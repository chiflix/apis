// Copyright 2018 Sagittarius LLC.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package sagittarius.translation.v1;

import "google/api/annotations.proto";
import "google/rpc/status.proto";
import "google/cloud/speech/v1/cloud_speech.proto";

option cc_enable_arenas = true;
option go_package = "translation";
option java_multiple_files = true;
option java_outer_classname = "TranslationProto";
option java_package = "ai.sagittarius.translation.v1";


// Service that implements Sagittarius Translation API
service Translation {
  // Translate media(audio or video) by media identity
  rpc TranslateMedia(MediaTranslationRequest) returns (MediaTranslationResponse) {
    option (google.api.http) = {
      post: "/v1/media/{media_identity}/language/{language_code}/transcript:{format}"
      body: "*"
    };
  }

  // detect the language of text
  rpc DetectLanguage(DetectionRequest) returns (DetectionResponse) {
    option (google.api.http) = {
      post: "/v1/language/detect"
      body: "text"
    };
  }

  rpc Transcript(TranscriptRequest) returns (TranscriptResponse) {
    option (google.api.http) = {
      post: "/v1/transcript/transcript_identity"
      body: "*"
    };
  }

  // Performs bidirectional streaming audio translation: receive results while
  // sending audio. This method is only available via the gRPC API (not REST).
  rpc StreamingTranslation(stream StreamingTranslationRequest)
    returns (stream StreamingTranslationResponse);

  // Translate text by Google Translation Service
  rpc TranslateText(TextTranslationRequest) returns (TextTranslationResponse) {
    option (google.api.http) = {
      post: "/v1/text/language/{target_language_code}/translation"
      body: "*"
    };
  }

  // Performs asynchronous audio translation: This method is only available via the gRPC API (not REST).
  rpc StreamingAsyTranslation(stream StreamingAsyTranslationRequest)
    returns (google.rpc.Status);
}

message MediaTranslationRequest {
  // Media Identity
  string media_identity = 1;

  // target language
  // ISO-639-1 Code https://cloud.google.com/translate/docs/languages
  string language_code  = 2;

  // the format of the transcripts
  string format         = 3;

  // position of the transcript relative to the begginning of the audio or video
  double start_time     = 6;

  // hints or keywords for more possible results
  string hints          = 7;
}

message MediaTranslationResponse {
  message TranscriptInfo {
    // the identity, can be used in TranslationRequest
    string transcript_identity = 1;

    // ISO-639-1 Code https://cloud.google.com/translate/docs/languages
    string language_code       = 2;

    float  ranking             = 3;

    repeated string tags       = 4;

    // in ms, can be +/-
    int64 delay                = 5;
  }

  // *Output-only* If set, returns a [google.rpc.Status][google.rpc.Status] message that
  // specifies the error for the operation.
  // return 404 if no result, in this case, client should use StreamingTranslationRequest
  google.rpc.Status error             = 1;

  // best translation results
  repeated TranscriptInfo results     = 2;
}

message DetectionRequest {
  // the text to be detect
  string text = 1;
}

message DetectionResponse {
  // there might be more than one prediction
  repeated DetectionPrediction prediction = 1;
}

message DetectionPrediction {
  // *Output-only* the language code of the detection result
  string language_code = 1;

  // *Output-only* The confidence estimate between 0.0 and 1.0. A higher number
  // indicates an estimated greater likelihood that the detection result are
  // correct.
  float  confidence    = 2;
}

message TranscriptRequest {
  // oneof case 2
  // return translate result by transcript_identity
  string transcript_identity  = 4;

  // position of the transcript relative to the begginning of the audio or video
  double start_time  = 6;
}

message TranscriptResponse {
  message Cue {
    // the start and end of the transcripts
    double start_time = 1;
    double end_time   = 2;

    string text = 3;
  }

  google.rpc.Status error               = 1;

  // each line of the transcript
  repeated Cue transcripts              = 2;

  // if transcripts ended in this result
  bool is_end_of_transcript             = 3;
}

// The top-level message sent by the client for the `StreamingRecognize` method.
// Multiple `StreamingTranslationRequest` messages are sent. The first message
// must contain a `streaming_config` message and must not contain `audio` data.
// All subsequent messages must contain `audio` data and must not contain a
// `streaming_config` message.
message StreamingTranslationRequest {
  // The streaming request, which is either a streaming config or audio content.
  oneof streaming_request {
    // Provides information to the recognizer that specifies how to process the
    // request. The first `StreamingTranslationRequest` message must contain a
    // `streaming_config`  message.
    google.cloud.speech.v1.RecognitionConfig streaming_config = 1;

    // The audio data to be recognized. Sequential chunks of audio data are sent
    // in sequential `StreamingTranslationRequest` messages. The first
    // `StreamingTranslationRequest` message must not contain `audio_content` data
    // and all subsequent `StreamingTranslationRequest` messages must contain
    // `audio_content` data. The audio bytes must be encoded as specified in
    // `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
    // pure binary representation (not base64). See
    // [audio limits](https://cloud.google.com/speech/limits#content).
    bytes audio_content = 2;
  }
  // the media identity
  string media_identity = 3;
}

message StreamingTranslationResponse {
  // *Output-only* If set, returns a [google.rpc.Status][google.rpc.Status] message that
  // specifies the error for the operation.
  google.rpc.Status error = 1;

  // *Output-only* This repeated list contains zero or more results that
  // correspond to consecutive portions of the audio currently being processed.
  // It contains zero or more `is_final=false` results followed by zero or one
  // `is_final=true` result (the newly settled portion).
  repeated StreamingTranslationResult results = 2;
}

message StreamingTranslationResult {
  // *Output-only* Transcript text representing the words that the user spoke.
  string transcript = 1;

  // *Output-only* The confidence estimate between 0.0 and 1.0. A higher number
  // indicates an estimated greater likelihood that the recognized words are
  // correct. This field is typically provided only for the top hypothesis, and
  // only for `is_final=true` results. Clients should not rely on the
  // `confidence` field as it is not guaranteed to be accurate or consistent.
  // The default of 0.0 is a sentinel value indicating `confidence` was not set.
  float confidence = 2;

  // *Output-only* An estimate of the likelihood that the recognizer will not
  // change its guess about this interim result. Values range from 0.0
  // (completely unstable) to 1.0 (completely stable).
  // This field is only provided for interim results (`is_final=false`).
  // The default of 0.0 is a sentinel value indicating `stability` was not set.
  float stability = 3;

  // *Output-only* Time offset relative to the beginning of the audio,
  // and corresponding to the start of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  // This is an experimental feature and the accuracy of the time offset can
  // vary.
  double start_time = 4;

  // *Output-only* Time offset relative to the beginning of the audio,
  // and corresponding to the end of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  // This is an experimental feature and the accuracy of the time offset can
  // vary.
  double end_time = 5;
}

// We use Google Translation Service for text translation.
// Translation API Reference: https://cloud.google.com/translate/docs/reference/translate
message TextTranslationRequest {
  // The text to translate
  repeated string text = 1;

  // The language to use for translation of the text
  // ISO-639-1/bcp47 Code with tags
  // https://cloud.google.com/translate/docs/languages
  string target_language_code = 2;

  // The language of the source text
  // ISO-639-1/bcp47 Code with tags
  // https://cloud.google.com/translate/docs/languages
  string source_language_code = 3;
}

message TextTranslationResponse {
  message Text {
    // The source language of the text detected
    // ISO-639-1/bcp47 Code with tags
    // https://cloud.google.com/translate/docs/languages
    string source_language = 1;

    // The translated Text
    string text = 2;
  }

  google.rpc.Status error = 1;

  // translation results for the requested text
  repeated Text results = 2;
}

message StreamingAsyTranslationRequest {
  // The streaming request, which is either a streaming config or audio content.
  oneof streaming_request {
    // Provides information to the recognizer that specifies how to process the
    // request. The first `StreamingTranslationRequest` message must contain a
    // `streaming_config`  message.
    google.cloud.speech.v1.RecognitionConfig streaming_config = 1;

    // The audio data to be trained. Sequential chunks of audio data are sent
    // in sequential `StreamingTranslationRequest` messages. The first
    // `StreamingTranslationRequest` message must not contain `audio_content` data
    // and all subsequent `StreamingTranslationRequest` messages must contain
    // `audio_content` data. The audio bytes must be encoded as specified in
    // `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
    // pure binary representation (not base64). See
    // [audio limits](https://cloud.google.com/speech/limits#content).
    bytes audio_content = 2;
  }

  // the media identity
  string media_identity        = 3;
}
