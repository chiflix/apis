// DO NOT EDIT.
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/speech/v1/cloud_speech.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2017 Google Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that your are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// The top-level message sent by the client for the `Recognize` method.
public struct Google_Cloud_Speech_V1_RecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1_RecognitionConfig {
    get {return _storage._config ?? Google_Cloud_Speech_V1_RecognitionConfig()}
    set {_uniqueStorage()._config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return _storage._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {_uniqueStorage()._config = nil}

  /// *Required* The audio data to be recognized.
  public var audio: Google_Cloud_Speech_V1_RecognitionAudio {
    get {return _storage._audio ?? Google_Cloud_Speech_V1_RecognitionAudio()}
    set {_uniqueStorage()._audio = newValue}
  }
  /// Returns true if `audio` has been explicitly set.
  public var hasAudio: Bool {return _storage._audio != nil}
  /// Clears the value of `audio`. Subsequent reads from it will return its default value.
  public mutating func clearAudio() {_uniqueStorage()._audio = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The top-level message sent by the client for the `LongRunningRecognize`
/// method.
public struct Google_Cloud_Speech_V1_LongRunningRecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1_RecognitionConfig {
    get {return _storage._config ?? Google_Cloud_Speech_V1_RecognitionConfig()}
    set {_uniqueStorage()._config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return _storage._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {_uniqueStorage()._config = nil}

  /// *Required* The audio data to be recognized.
  public var audio: Google_Cloud_Speech_V1_RecognitionAudio {
    get {return _storage._audio ?? Google_Cloud_Speech_V1_RecognitionAudio()}
    set {_uniqueStorage()._audio = newValue}
  }
  /// Returns true if `audio` has been explicitly set.
  public var hasAudio: Bool {return _storage._audio != nil}
  /// Clears the value of `audio`. Subsequent reads from it will return its default value.
  public mutating func clearAudio() {_uniqueStorage()._audio = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The top-level message sent by the client for the `StreamingRecognize` method.
/// Multiple `StreamingRecognizeRequest` messages are sent. The first message
/// must contain a `streaming_config` message and must not contain `audio` data.
/// All subsequent messages must contain `audio` data and must not contain a
/// `streaming_config` message.
public struct Google_Cloud_Speech_V1_StreamingRecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The streaming request, which is either a streaming config or audio content.
  public var streamingRequest: OneOf_StreamingRequest? {
    get {return _storage._streamingRequest}
    set {_uniqueStorage()._streamingRequest = newValue}
  }

  /// Provides information to the recognizer that specifies how to process the
  /// request. The first `StreamingRecognizeRequest` message must contain a
  /// `streaming_config`  message.
  public var streamingConfig: Google_Cloud_Speech_V1_StreamingRecognitionConfig {
    get {
      if case .streamingConfig(let v)? = _storage._streamingRequest {return v}
      return Google_Cloud_Speech_V1_StreamingRecognitionConfig()
    }
    set {_uniqueStorage()._streamingRequest = .streamingConfig(newValue)}
  }

  /// The audio data to be recognized. Sequential chunks of audio data are sent
  /// in sequential `StreamingRecognizeRequest` messages. The first
  /// `StreamingRecognizeRequest` message must not contain `audio_content` data
  /// and all subsequent `StreamingRecognizeRequest` messages must contain
  /// `audio_content` data. The audio bytes must be encoded as specified in
  /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
  /// pure binary representation (not base64). See
  /// [audio limits](https://cloud.google.com/speech/limits#content).
  public var audioContent: Data {
    get {
      if case .audioContent(let v)? = _storage._streamingRequest {return v}
      return SwiftProtobuf.Internal.emptyData
    }
    set {_uniqueStorage()._streamingRequest = .audioContent(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The streaming request, which is either a streaming config or audio content.
  public enum OneOf_StreamingRequest: Equatable {
    /// Provides information to the recognizer that specifies how to process the
    /// request. The first `StreamingRecognizeRequest` message must contain a
    /// `streaming_config`  message.
    case streamingConfig(Google_Cloud_Speech_V1_StreamingRecognitionConfig)
    /// The audio data to be recognized. Sequential chunks of audio data are sent
    /// in sequential `StreamingRecognizeRequest` messages. The first
    /// `StreamingRecognizeRequest` message must not contain `audio_content` data
    /// and all subsequent `StreamingRecognizeRequest` messages must contain
    /// `audio_content` data. The audio bytes must be encoded as specified in
    /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
    /// pure binary representation (not base64). See
    /// [audio limits](https://cloud.google.com/speech/limits#content).
    case audioContent(Data)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest.OneOf_StreamingRequest, rhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest.OneOf_StreamingRequest) -> Bool {
      switch (lhs, rhs) {
      case (.streamingConfig(let l), .streamingConfig(let r)): return l == r
      case (.audioContent(let l), .audioContent(let r)): return l == r
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Provides information to the recognizer that specifies how to process the
/// request.
public struct Google_Cloud_Speech_V1_StreamingRecognitionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1_RecognitionConfig {
    get {return _storage._config ?? Google_Cloud_Speech_V1_RecognitionConfig()}
    set {_uniqueStorage()._config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return _storage._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {_uniqueStorage()._config = nil}

  /// *Optional* If `false` or omitted, the recognizer will perform continuous
  /// recognition (continuing to wait for and process audio even if the user
  /// pauses speaking) until the client closes the input stream (gRPC API) or
  /// until the maximum time limit has been reached. May return multiple
  /// `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
  ///
  /// If `true`, the recognizer will detect a single spoken utterance. When it
  /// detects that the user has paused or stopped speaking, it will return an
  /// `END_OF_SINGLE_UTTERANCE` event and cease recognition. It will return no
  /// more than one `StreamingRecognitionResult` with the `is_final` flag set to
  /// `true`.
  public var singleUtterance: Bool {
    get {return _storage._singleUtterance}
    set {_uniqueStorage()._singleUtterance = newValue}
  }

  /// *Optional* If `true`, interim results (tentative hypotheses) may be
  /// returned as they become available (these interim results are indicated with
  /// the `is_final=false` flag).
  /// If `false` or omitted, only `is_final=true` result(s) are returned.
  public var interimResults: Bool {
    get {return _storage._interimResults}
    set {_uniqueStorage()._interimResults = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Provides information to the recognizer that specifies how to process the
/// request.
public struct Google_Cloud_Speech_V1_RecognitionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
  public var encoding: Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding = .encodingUnspecified

  /// *Required* Sample rate in Hertz of the audio data sent in all
  /// `RecognitionAudio` messages. Valid values are: 8000-48000.
  /// 16000 is optimal. For best results, set the sampling rate of the audio
  /// source to 16000 Hz. If that's not possible, use the native sample rate of
  /// the audio source (instead of re-sampling).
  public var sampleRateHertz: Int32 = 0

  /// *Required* The language of the supplied audio as a
  /// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  /// Example: "en-US".
  /// See [Language Support](https://cloud.google.com/speech/docs/languages)
  /// for a list of the currently supported language codes.
  public var languageCode: String = String()

  /// *Optional* Maximum number of recognition hypotheses to be returned.
  /// Specifically, the maximum number of `SpeechRecognitionAlternative` messages
  /// within each `SpeechRecognitionResult`.
  /// The server may return fewer than `max_alternatives`.
  /// Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
  /// one. If omitted, will return a maximum of one.
  public var maxAlternatives: Int32 = 0

  /// *Optional* If set to `true`, the server will attempt to filter out
  /// profanities, replacing all but the initial character in each filtered word
  /// with asterisks, e.g. "f***". If set to `false` or omitted, profanities
  /// won't be filtered out.
  public var profanityFilter: Bool = false

  /// *Optional* A means to provide context to assist the speech recognition.
  public var speechContexts: [Google_Cloud_Speech_V1_SpeechContext] = []

  /// *Optional* If `true`, the top result includes a list of words and
  /// the start and end time offsets (timestamps) for those words. If
  /// `false`, no word-level time offset information is returned. The default is
  /// `false`.
  public var enableWordTimeOffsets: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Audio encoding of the data sent in the audio message. All encodings support
  /// only 1 channel (mono) audio. Only `FLAC` and `WAV` include a header that
  /// describes the bytes of audio that follow the header. The other encodings
  /// are raw audio bytes with no header.
  ///
  /// For best results, the audio source should be captured and transmitted using
  /// a lossless encoding (`FLAC` or `LINEAR16`). Recognition accuracy may be
  /// reduced if lossy codecs, which include the other codecs listed in
  /// this section, are used to capture or transmit the audio, particularly if
  /// background noise is present.
  public enum AudioEncoding: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
    case encodingUnspecified // = 0

    /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
    case linear16 // = 1

    /// [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
    /// Codec) is the recommended encoding because it is
    /// lossless--therefore recognition is not compromised--and
    /// requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
    /// encoding supports 16-bit and 24-bit samples, however, not all fields in
    /// `STREAMINFO` are supported.
    case flac // = 2

    /// 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    case mulaw // = 3

    /// Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
    case amr // = 4

    /// Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
    case amrWb // = 5

    /// Opus encoded audio frames in Ogg container
    /// ([OggOpus](https://wiki.xiph.org/OggOpus)).
    /// `sample_rate_hertz` must be 16000.
    case oggOpus // = 6

    /// Although the use of lossy encodings is not recommended, if a very low
    /// bitrate encoding is required, `OGG_OPUS` is highly preferred over
    /// Speex encoding. The [Speex](https://speex.org/)  encoding supported by
    /// Cloud Speech API has a header byte in each block, as in MIME type
    /// `audio/x-speex-with-header-byte`.
    /// It is a variant of the RTP Speex encoding defined in
    /// [RFC 5574](https://tools.ietf.org/html/rfc5574).
    /// The stream is a sequence of blocks, one block per RTP packet. Each block
    /// starts with a byte containing the length of the block, in bytes, followed
    /// by one or more frames of Speex data, padded to an integral number of
    /// bytes (octets) as specified in RFC 5574. In other words, each RTP header
    /// is replaced with a single byte containing the block length. Only Speex
    /// wideband is supported. `sample_rate_hertz` must be 16000.
    case speexWithHeaderByte // = 7
    case UNRECOGNIZED(Int)

    public init() {
      self = .encodingUnspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .encodingUnspecified
      case 1: self = .linear16
      case 2: self = .flac
      case 3: self = .mulaw
      case 4: self = .amr
      case 5: self = .amrWb
      case 6: self = .oggOpus
      case 7: self = .speexWithHeaderByte
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .encodingUnspecified: return 0
      case .linear16: return 1
      case .flac: return 2
      case .mulaw: return 3
      case .amr: return 4
      case .amrWb: return 5
      case .oggOpus: return 6
      case .speexWithHeaderByte: return 7
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding] = [
    .encodingUnspecified,
    .linear16,
    .flac,
    .mulaw,
    .amr,
    .amrWb,
    .oggOpus,
    .speexWithHeaderByte,
  ]
}

#endif  // swift(>=4.2)

/// Provides "hints" to the speech recognizer to favor specific words and phrases
/// in the results.
public struct Google_Cloud_Speech_V1_SpeechContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Optional* A list of strings containing words and phrases "hints" so that
  /// the speech recognition is more likely to recognize them. This can be used
  /// to improve the accuracy for specific words and phrases, for example, if
  /// specific commands are typically spoken by the user. This can also be used
  /// to add additional words to the vocabulary of the recognizer. See
  /// [usage limits](https://cloud.google.com/speech/limits#content).
  public var phrases: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Contains audio data in the encoding specified in the `RecognitionConfig`.
/// Either `content` or `uri` must be supplied. Supplying both or neither
/// returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
/// [audio limits](https://cloud.google.com/speech/limits#content).
public struct Google_Cloud_Speech_V1_RecognitionAudio {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The audio source, which is either inline content or a Google Cloud
  /// Storage uri.
  public var audioSource: Google_Cloud_Speech_V1_RecognitionAudio.OneOf_AudioSource? = nil

  /// The audio data bytes encoded as specified in
  /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
  /// pure binary representation, whereas JSON representations use base64.
  public var content: Data {
    get {
      if case .content(let v)? = audioSource {return v}
      return SwiftProtobuf.Internal.emptyData
    }
    set {audioSource = .content(newValue)}
  }

  /// URI that points to a file that contains audio data bytes as specified in
  /// `RecognitionConfig`. Currently, only Google Cloud Storage URIs are
  /// supported, which must be specified in the following format:
  /// `gs://bucket_name/object_name` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
  /// [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
  public var uri: String {
    get {
      if case .uri(let v)? = audioSource {return v}
      return String()
    }
    set {audioSource = .uri(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The audio source, which is either inline content or a Google Cloud
  /// Storage uri.
  public enum OneOf_AudioSource: Equatable {
    /// The audio data bytes encoded as specified in
    /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
    /// pure binary representation, whereas JSON representations use base64.
    case content(Data)
    /// URI that points to a file that contains audio data bytes as specified in
    /// `RecognitionConfig`. Currently, only Google Cloud Storage URIs are
    /// supported, which must be specified in the following format:
    /// `gs://bucket_name/object_name` (other URI formats return
    /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
    /// [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
    case uri(String)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Speech_V1_RecognitionAudio.OneOf_AudioSource, rhs: Google_Cloud_Speech_V1_RecognitionAudio.OneOf_AudioSource) -> Bool {
      switch (lhs, rhs) {
      case (.content(let l), .content(let r)): return l == r
      case (.uri(let l), .uri(let r)): return l == r
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// The only message returned to the client by the `Recognize` method. It
/// contains the result as zero or more sequential `SpeechRecognitionResult`
/// messages.
public struct Google_Cloud_Speech_V1_RecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* Sequential list of transcription results corresponding to
  /// sequential portions of audio.
  public var results: [Google_Cloud_Speech_V1_SpeechRecognitionResult] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The only message returned to the client by the `LongRunningRecognize` method.
/// It contains the result as zero or more sequential `SpeechRecognitionResult`
/// messages. It is included in the `result.response` field of the `Operation`
/// returned by the `GetOperation` call of the `google::longrunning::Operations`
/// service.
public struct Google_Cloud_Speech_V1_LongRunningRecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* Sequential list of transcription results corresponding to
  /// sequential portions of audio.
  public var results: [Google_Cloud_Speech_V1_SpeechRecognitionResult] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Describes the progress of a long-running `LongRunningRecognize` call. It is
/// included in the `metadata` field of the `Operation` returned by the
/// `GetOperation` call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Speech_V1_LongRunningRecognizeMetadata {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Approximate percentage of audio processed thus far. Guaranteed to be 100
  /// when the audio is fully processed and the results are available.
  public var progressPercent: Int32 {
    get {return _storage._progressPercent}
    set {_uniqueStorage()._progressPercent = newValue}
  }

  /// Time when the request was received.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Time of the most recent processing update.
  public var lastUpdateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._lastUpdateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._lastUpdateTime = newValue}
  }
  /// Returns true if `lastUpdateTime` has been explicitly set.
  public var hasLastUpdateTime: Bool {return _storage._lastUpdateTime != nil}
  /// Clears the value of `lastUpdateTime`. Subsequent reads from it will return its default value.
  public mutating func clearLastUpdateTime() {_uniqueStorage()._lastUpdateTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// `StreamingRecognizeResponse` is the only message returned to the client by
/// `StreamingRecognize`. A series of zero or more `StreamingRecognizeResponse`
/// messages are streamed back to the client. If there is no recognizable
/// audio, and `single_utterance` is set to false, then no messages are streamed
/// back to the client.
///
/// Here's an example of a series of ten `StreamingRecognizeResponse`s that might
/// be returned while processing audio:
///
/// 1. results { alternatives { transcript: "tube" } stability: 0.01 }
///
/// 2. results { alternatives { transcript: "to be a" } stability: 0.01 }
///
/// 3. results { alternatives { transcript: "to be" } stability: 0.9 }
///    results { alternatives { transcript: " or not to be" } stability: 0.01 }
///
/// 4. results { alternatives { transcript: "to be or not to be"
///                             confidence: 0.92 }
///              alternatives { transcript: "to bee or not to bee" }
///              is_final: true }
///
/// 5. results { alternatives { transcript: " that's" } stability: 0.01 }
///
/// 6. results { alternatives { transcript: " that is" } stability: 0.9 }
///    results { alternatives { transcript: " the question" } stability: 0.01 }
///
/// 7. results { alternatives { transcript: " that is the question"
///                             confidence: 0.98 }
///              alternatives { transcript: " that was the question" }
///              is_final: true }
///
/// Notes:
///
/// - Only two of the above responses #4 and #7 contain final results; they are
///   indicated by `is_final: true`. Concatenating these together generates the
///   full transcript: "to be or not to be that is the question".
///
/// - The others contain interim `results`. #3 and #6 contain two interim
///   `results`: the first portion has a high stability and is less likely to
///   change; the second portion has a low stability and is very likely to
///   change. A UI designer might choose to show only high stability `results`.
///
/// - The specific `stability` and `confidence` values shown above are only for
///   illustrative purposes. Actual values may vary.
///
/// - In each response, only one of these fields will be set:
///     `error`,
///     `speech_event_type`, or
///     one or more (repeated) `results`.
public struct Google_Cloud_Speech_V1_StreamingRecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* If set, returns a [google.rpc.Status][google.rpc.Status] message that
  /// specifies the error for the operation.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// *Output-only* This repeated list contains zero or more results that
  /// correspond to consecutive portions of the audio currently being processed.
  /// It contains zero or more `is_final=false` results followed by zero or one
  /// `is_final=true` result (the newly settled portion).
  public var results: [Google_Cloud_Speech_V1_StreamingRecognitionResult] {
    get {return _storage._results}
    set {_uniqueStorage()._results = newValue}
  }

  /// *Output-only* Indicates the type of speech event.
  public var speechEventType: Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType {
    get {return _storage._speechEventType}
    set {_uniqueStorage()._speechEventType = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates the type of speech event.
  public enum SpeechEventType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// No speech event specified.
    case speechEventUnspecified // = 0

    /// This event indicates that the server has detected the end of the user's
    /// speech utterance and expects no additional speech. Therefore, the server
    /// will not process additional audio (although it may subsequently return
    /// additional results). The client should stop sending additional audio
    /// data, half-close the gRPC connection, and wait for any additional results
    /// until the server closes the gRPC connection. This event is only sent if
    /// `single_utterance` was set to `true`, and is not used otherwise.
    case endOfSingleUtterance // = 1
    case UNRECOGNIZED(Int)

    public init() {
      self = .speechEventUnspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .speechEventUnspecified
      case 1: self = .endOfSingleUtterance
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .speechEventUnspecified: return 0
      case .endOfSingleUtterance: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

#if swift(>=4.2)

extension Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType] = [
    .speechEventUnspecified,
    .endOfSingleUtterance,
  ]
}

#endif  // swift(>=4.2)

/// A streaming speech recognition result corresponding to a portion of the audio
/// that is currently being processed.
public struct Google_Cloud_Speech_V1_StreamingRecognitionResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* May contain one or more recognition hypotheses (up to the
  /// maximum specified in `max_alternatives`).
  public var alternatives: [Google_Cloud_Speech_V1_SpeechRecognitionAlternative] = []

  /// *Output-only* If `false`, this `StreamingRecognitionResult` represents an
  /// interim result that may change. If `true`, this is the final time the
  /// speech service will return this particular `StreamingRecognitionResult`,
  /// the recognizer will not return any further hypotheses for this portion of
  /// the transcript and corresponding audio.
  public var isFinal: Bool = false

  /// *Output-only* An estimate of the likelihood that the recognizer will not
  /// change its guess about this interim result. Values range from 0.0
  /// (completely unstable) to 1.0 (completely stable).
  /// This field is only provided for interim results (`is_final=false`).
  /// The default of 0.0 is a sentinel value indicating `stability` was not set.
  public var stability: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A speech recognition result corresponding to a portion of the audio.
public struct Google_Cloud_Speech_V1_SpeechRecognitionResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* May contain one or more recognition hypotheses (up to the
  /// maximum specified in `max_alternatives`).
  /// These alternatives are ordered in terms of accuracy, with the top (first)
  /// alternative being the most probable, as ranked by the recognizer.
  public var alternatives: [Google_Cloud_Speech_V1_SpeechRecognitionAlternative] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Alternative hypotheses (a.k.a. n-best list).
public struct Google_Cloud_Speech_V1_SpeechRecognitionAlternative {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* Transcript text representing the words that the user spoke.
  public var transcript: String = String()

  /// *Output-only* The confidence estimate between 0.0 and 1.0. A higher number
  /// indicates an estimated greater likelihood that the recognized words are
  /// correct. This field is typically provided only for the top hypothesis, and
  /// only for `is_final=true` results. Clients should not rely on the
  /// `confidence` field as it is not guaranteed to be accurate or consistent.
  /// The default of 0.0 is a sentinel value indicating `confidence` was not set.
  public var confidence: Float = 0

  /// *Output-only* A list of word-specific information for each recognized word.
  public var words: [Google_Cloud_Speech_V1_WordInfo] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Word-specific information for recognized words. Word information is only
/// included in the response when certain request parameters are set, such
/// as `enable_word_time_offsets`.
public struct Google_Cloud_Speech_V1_WordInfo {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* Time offset relative to the beginning of the audio,
  /// and corresponding to the start of the spoken word.
  /// This field is only set if `enable_word_time_offsets=true` and only
  /// in the top hypothesis.
  /// This is an experimental feature and the accuracy of the time offset can
  /// vary.
  public var startTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// *Output-only* Time offset relative to the beginning of the audio,
  /// and corresponding to the end of the spoken word.
  /// This field is only set if `enable_word_time_offsets=true` and only
  /// in the top hypothesis.
  /// This is an experimental feature and the accuracy of the time offset can
  /// vary.
  public var endTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// *Output-only* The word corresponding to this set of information.
  public var word: String {
    get {return _storage._word}
    set {_uniqueStorage()._word = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.speech.v1"

extension Google_Cloud_Speech_V1_RecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .same(proto: "audio"),
  ]

  fileprivate class _StorageClass {
    var _config: Google_Cloud_Speech_V1_RecognitionConfig? = nil
    var _audio: Google_Cloud_Speech_V1_RecognitionAudio? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _config = source._config
      _audio = source._audio
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._config)
        case 2: try decoder.decodeSingularMessageField(value: &_storage._audio)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._config {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if let v = _storage._audio {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognizeRequest, rhs: Google_Cloud_Speech_V1_RecognizeRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._config != rhs_storage._config {return false}
        if _storage._audio != rhs_storage._audio {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_LongRunningRecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LongRunningRecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .same(proto: "audio"),
  ]

  fileprivate class _StorageClass {
    var _config: Google_Cloud_Speech_V1_RecognitionConfig? = nil
    var _audio: Google_Cloud_Speech_V1_RecognitionAudio? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _config = source._config
      _audio = source._audio
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._config)
        case 2: try decoder.decodeSingularMessageField(value: &_storage._audio)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._config {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if let v = _storage._audio {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_LongRunningRecognizeRequest, rhs: Google_Cloud_Speech_V1_LongRunningRecognizeRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._config != rhs_storage._config {return false}
        if _storage._audio != rhs_storage._audio {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "streaming_config"),
    2: .standard(proto: "audio_content"),
  ]

  fileprivate class _StorageClass {
    var _streamingRequest: Google_Cloud_Speech_V1_StreamingRecognizeRequest.OneOf_StreamingRequest?

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _streamingRequest = source._streamingRequest
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1:
          var v: Google_Cloud_Speech_V1_StreamingRecognitionConfig?
          if let current = _storage._streamingRequest {
            try decoder.handleConflictingOneOf()
            if case .streamingConfig(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._streamingRequest = .streamingConfig(v)}
        case 2:
          if _storage._streamingRequest != nil {try decoder.handleConflictingOneOf()}
          var v: Data?
          try decoder.decodeSingularBytesField(value: &v)
          if let v = v {_storage._streamingRequest = .audioContent(v)}
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      switch _storage._streamingRequest {
      case .streamingConfig(let v)?:
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      case .audioContent(let v)?:
        try visitor.visitSingularBytesField(value: v, fieldNumber: 2)
      case nil: break
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest, rhs: Google_Cloud_Speech_V1_StreamingRecognizeRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._streamingRequest != rhs_storage._streamingRequest {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognitionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognitionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .standard(proto: "single_utterance"),
    3: .standard(proto: "interim_results"),
  ]

  fileprivate class _StorageClass {
    var _config: Google_Cloud_Speech_V1_RecognitionConfig? = nil
    var _singleUtterance: Bool = false
    var _interimResults: Bool = false

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _config = source._config
      _singleUtterance = source._singleUtterance
      _interimResults = source._interimResults
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._config)
        case 2: try decoder.decodeSingularBoolField(value: &_storage._singleUtterance)
        case 3: try decoder.decodeSingularBoolField(value: &_storage._interimResults)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._config {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if _storage._singleUtterance != false {
        try visitor.visitSingularBoolField(value: _storage._singleUtterance, fieldNumber: 2)
      }
      if _storage._interimResults != false {
        try visitor.visitSingularBoolField(value: _storage._interimResults, fieldNumber: 3)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognitionConfig, rhs: Google_Cloud_Speech_V1_StreamingRecognitionConfig) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._config != rhs_storage._config {return false}
        if _storage._singleUtterance != rhs_storage._singleUtterance {return false}
        if _storage._interimResults != rhs_storage._interimResults {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognitionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "encoding"),
    2: .standard(proto: "sample_rate_hertz"),
    3: .standard(proto: "language_code"),
    4: .standard(proto: "max_alternatives"),
    5: .standard(proto: "profanity_filter"),
    6: .standard(proto: "speech_contexts"),
    8: .standard(proto: "enable_word_time_offsets"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularEnumField(value: &self.encoding)
      case 2: try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz)
      case 3: try decoder.decodeSingularStringField(value: &self.languageCode)
      case 4: try decoder.decodeSingularInt32Field(value: &self.maxAlternatives)
      case 5: try decoder.decodeSingularBoolField(value: &self.profanityFilter)
      case 6: try decoder.decodeRepeatedMessageField(value: &self.speechContexts)
      case 8: try decoder.decodeSingularBoolField(value: &self.enableWordTimeOffsets)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.encoding != .encodingUnspecified {
      try visitor.visitSingularEnumField(value: self.encoding, fieldNumber: 1)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 2)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 3)
    }
    if self.maxAlternatives != 0 {
      try visitor.visitSingularInt32Field(value: self.maxAlternatives, fieldNumber: 4)
    }
    if self.profanityFilter != false {
      try visitor.visitSingularBoolField(value: self.profanityFilter, fieldNumber: 5)
    }
    if !self.speechContexts.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.speechContexts, fieldNumber: 6)
    }
    if self.enableWordTimeOffsets != false {
      try visitor.visitSingularBoolField(value: self.enableWordTimeOffsets, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognitionConfig, rhs: Google_Cloud_Speech_V1_RecognitionConfig) -> Bool {
    if lhs.encoding != rhs.encoding {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.maxAlternatives != rhs.maxAlternatives {return false}
    if lhs.profanityFilter != rhs.profanityFilter {return false}
    if lhs.speechContexts != rhs.speechContexts {return false}
    if lhs.enableWordTimeOffsets != rhs.enableWordTimeOffsets {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionConfig.AudioEncoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ENCODING_UNSPECIFIED"),
    1: .same(proto: "LINEAR16"),
    2: .same(proto: "FLAC"),
    3: .same(proto: "MULAW"),
    4: .same(proto: "AMR"),
    5: .same(proto: "AMR_WB"),
    6: .same(proto: "OGG_OPUS"),
    7: .same(proto: "SPEEX_WITH_HEADER_BYTE"),
  ]
}

extension Google_Cloud_Speech_V1_SpeechContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "phrases"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedStringField(value: &self.phrases)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.phrases.isEmpty {
      try visitor.visitRepeatedStringField(value: self.phrases, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_SpeechContext, rhs: Google_Cloud_Speech_V1_SpeechContext) -> Bool {
    if lhs.phrases != rhs.phrases {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognitionAudio: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognitionAudio"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "content"),
    2: .same(proto: "uri"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1:
        if self.audioSource != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.audioSource = .content(v)}
      case 2:
        if self.audioSource != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.audioSource = .uri(v)}
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    switch self.audioSource {
    case .content(let v)?:
      try visitor.visitSingularBytesField(value: v, fieldNumber: 1)
    case .uri(let v)?:
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognitionAudio, rhs: Google_Cloud_Speech_V1_RecognitionAudio) -> Bool {
    if lhs.audioSource != rhs.audioSource {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_RecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 2: try decoder.decodeRepeatedMessageField(value: &self.results)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.results.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.results, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_RecognizeResponse, rhs: Google_Cloud_Speech_V1_RecognizeResponse) -> Bool {
    if lhs.results != rhs.results {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_LongRunningRecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LongRunningRecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 2: try decoder.decodeRepeatedMessageField(value: &self.results)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.results.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.results, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_LongRunningRecognizeResponse, rhs: Google_Cloud_Speech_V1_LongRunningRecognizeResponse) -> Bool {
    if lhs.results != rhs.results {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_LongRunningRecognizeMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LongRunningRecognizeMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "progress_percent"),
    2: .standard(proto: "start_time"),
    3: .standard(proto: "last_update_time"),
  ]

  fileprivate class _StorageClass {
    var _progressPercent: Int32 = 0
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _lastUpdateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _progressPercent = source._progressPercent
      _startTime = source._startTime
      _lastUpdateTime = source._lastUpdateTime
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularInt32Field(value: &_storage._progressPercent)
        case 2: try decoder.decodeSingularMessageField(value: &_storage._startTime)
        case 3: try decoder.decodeSingularMessageField(value: &_storage._lastUpdateTime)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if _storage._progressPercent != 0 {
        try visitor.visitSingularInt32Field(value: _storage._progressPercent, fieldNumber: 1)
      }
      if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
      if let v = _storage._lastUpdateTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_LongRunningRecognizeMetadata, rhs: Google_Cloud_Speech_V1_LongRunningRecognizeMetadata) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._progressPercent != rhs_storage._progressPercent {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._lastUpdateTime != rhs_storage._lastUpdateTime {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "error"),
    2: .same(proto: "results"),
    4: .standard(proto: "speech_event_type"),
  ]

  fileprivate class _StorageClass {
    var _error: Google_Rpc_Status? = nil
    var _results: [Google_Cloud_Speech_V1_StreamingRecognitionResult] = []
    var _speechEventType: Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType = .speechEventUnspecified

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _error = source._error
      _results = source._results
      _speechEventType = source._speechEventType
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._error)
        case 2: try decoder.decodeRepeatedMessageField(value: &_storage._results)
        case 4: try decoder.decodeSingularEnumField(value: &_storage._speechEventType)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if !_storage._results.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._results, fieldNumber: 2)
      }
      if _storage._speechEventType != .speechEventUnspecified {
        try visitor.visitSingularEnumField(value: _storage._speechEventType, fieldNumber: 4)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognizeResponse, rhs: Google_Cloud_Speech_V1_StreamingRecognizeResponse) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._error != rhs_storage._error {return false}
        if _storage._results != rhs_storage._results {return false}
        if _storage._speechEventType != rhs_storage._speechEventType {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_StreamingRecognizeResponse.SpeechEventType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SPEECH_EVENT_UNSPECIFIED"),
    1: .same(proto: "END_OF_SINGLE_UTTERANCE"),
  ]
}

extension Google_Cloud_Speech_V1_StreamingRecognitionResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognitionResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
    2: .standard(proto: "is_final"),
    3: .same(proto: "stability"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedMessageField(value: &self.alternatives)
      case 2: try decoder.decodeSingularBoolField(value: &self.isFinal)
      case 3: try decoder.decodeSingularFloatField(value: &self.stability)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    if self.isFinal != false {
      try visitor.visitSingularBoolField(value: self.isFinal, fieldNumber: 2)
    }
    if self.stability != 0 {
      try visitor.visitSingularFloatField(value: self.stability, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_StreamingRecognitionResult, rhs: Google_Cloud_Speech_V1_StreamingRecognitionResult) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.isFinal != rhs.isFinal {return false}
    if lhs.stability != rhs.stability {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_SpeechRecognitionResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedMessageField(value: &self.alternatives)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_SpeechRecognitionResult, rhs: Google_Cloud_Speech_V1_SpeechRecognitionResult) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_SpeechRecognitionAlternative: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionAlternative"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "transcript"),
    2: .same(proto: "confidence"),
    3: .same(proto: "words"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularStringField(value: &self.transcript)
      case 2: try decoder.decodeSingularFloatField(value: &self.confidence)
      case 3: try decoder.decodeRepeatedMessageField(value: &self.words)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.transcript.isEmpty {
      try visitor.visitSingularStringField(value: self.transcript, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    if !self.words.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.words, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_SpeechRecognitionAlternative, rhs: Google_Cloud_Speech_V1_SpeechRecognitionAlternative) -> Bool {
    if lhs.transcript != rhs.transcript {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.words != rhs.words {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1_WordInfo: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WordInfo"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time"),
    2: .standard(proto: "end_time"),
    3: .same(proto: "word"),
  ]

  fileprivate class _StorageClass {
    var _startTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
    var _word: String = String()

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _startTime = source._startTime
      _endTime = source._endTime
      _word = source._word
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._startTime)
        case 2: try decoder.decodeSingularMessageField(value: &_storage._endTime)
        case 3: try decoder.decodeSingularStringField(value: &_storage._word)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
      if !_storage._word.isEmpty {
        try visitor.visitSingularStringField(value: _storage._word, fieldNumber: 3)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1_WordInfo, rhs: Google_Cloud_Speech_V1_WordInfo) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._word != rhs_storage._word {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
