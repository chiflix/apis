// DO NOT EDIT.
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/speech/v1beta1/cloud_speech.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2017 Google Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

import GoogleAPIModel
import GoogleLongrunningModel
import GoogleRPCModel

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that your are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// The top-level message sent by the client for the `SyncRecognize` method.
public struct Google_Cloud_Speech_V1beta1_SyncRecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1beta1_RecognitionConfig {
    get {return _storage._config ?? Google_Cloud_Speech_V1beta1_RecognitionConfig()}
    set {_uniqueStorage()._config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return _storage._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {_uniqueStorage()._config = nil}

  /// *Required* The audio data to be recognized.
  public var audio: Google_Cloud_Speech_V1beta1_RecognitionAudio {
    get {return _storage._audio ?? Google_Cloud_Speech_V1beta1_RecognitionAudio()}
    set {_uniqueStorage()._audio = newValue}
  }
  /// Returns true if `audio` has been explicitly set.
  public var hasAudio: Bool {return _storage._audio != nil}
  /// Clears the value of `audio`. Subsequent reads from it will return its default value.
  public mutating func clearAudio() {_uniqueStorage()._audio = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The top-level message sent by the client for the `AsyncRecognize` method.
public struct Google_Cloud_Speech_V1beta1_AsyncRecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1beta1_RecognitionConfig {
    get {return _storage._config ?? Google_Cloud_Speech_V1beta1_RecognitionConfig()}
    set {_uniqueStorage()._config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return _storage._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {_uniqueStorage()._config = nil}

  /// *Required* The audio data to be recognized.
  public var audio: Google_Cloud_Speech_V1beta1_RecognitionAudio {
    get {return _storage._audio ?? Google_Cloud_Speech_V1beta1_RecognitionAudio()}
    set {_uniqueStorage()._audio = newValue}
  }
  /// Returns true if `audio` has been explicitly set.
  public var hasAudio: Bool {return _storage._audio != nil}
  /// Clears the value of `audio`. Subsequent reads from it will return its default value.
  public mutating func clearAudio() {_uniqueStorage()._audio = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The top-level message sent by the client for the `StreamingRecognize` method.
/// Multiple `StreamingRecognizeRequest` messages are sent. The first message
/// must contain a `streaming_config` message and must not contain `audio` data.
/// All subsequent messages must contain `audio` data and must not contain a
/// `streaming_config` message.
public struct Google_Cloud_Speech_V1beta1_StreamingRecognizeRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The streaming request, which is either a streaming config or audio content.
  public var streamingRequest: OneOf_StreamingRequest? {
    get {return _storage._streamingRequest}
    set {_uniqueStorage()._streamingRequest = newValue}
  }

  /// Provides information to the recognizer that specifies how to process the
  /// request. The first `StreamingRecognizeRequest` message must contain a
  /// `streaming_config`  message.
  public var streamingConfig: Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig {
    get {
      if case .streamingConfig(let v)? = _storage._streamingRequest {return v}
      return Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig()
    }
    set {_uniqueStorage()._streamingRequest = .streamingConfig(newValue)}
  }

  /// The audio data to be recognized. Sequential chunks of audio data are sent
  /// in sequential `StreamingRecognizeRequest` messages. The first
  /// `StreamingRecognizeRequest` message must not contain `audio_content` data
  /// and all subsequent `StreamingRecognizeRequest` messages must contain
  /// `audio_content` data. The audio bytes must be encoded as specified in
  /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
  /// pure binary representation (not base64). See
  /// [audio limits](https://cloud.google.com/speech/limits#content).
  public var audioContent: Data {
    get {
      if case .audioContent(let v)? = _storage._streamingRequest {return v}
      return SwiftProtobuf.Internal.emptyData
    }
    set {_uniqueStorage()._streamingRequest = .audioContent(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The streaming request, which is either a streaming config or audio content.
  public enum OneOf_StreamingRequest: Equatable {
    /// Provides information to the recognizer that specifies how to process the
    /// request. The first `StreamingRecognizeRequest` message must contain a
    /// `streaming_config`  message.
    case streamingConfig(Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig)
    /// The audio data to be recognized. Sequential chunks of audio data are sent
    /// in sequential `StreamingRecognizeRequest` messages. The first
    /// `StreamingRecognizeRequest` message must not contain `audio_content` data
    /// and all subsequent `StreamingRecognizeRequest` messages must contain
    /// `audio_content` data. The audio bytes must be encoded as specified in
    /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
    /// pure binary representation (not base64). See
    /// [audio limits](https://cloud.google.com/speech/limits#content).
    case audioContent(Data)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Speech_V1beta1_StreamingRecognizeRequest.OneOf_StreamingRequest, rhs: Google_Cloud_Speech_V1beta1_StreamingRecognizeRequest.OneOf_StreamingRequest) -> Bool {
      switch (lhs, rhs) {
      case (.streamingConfig(let l), .streamingConfig(let r)): return l == r
      case (.audioContent(let l), .audioContent(let r)): return l == r
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Provides information to the recognizer that specifies how to process the
/// request.
public struct Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Provides information to the recognizer that specifies how to
  /// process the request.
  public var config: Google_Cloud_Speech_V1beta1_RecognitionConfig {
    get {return _storage._config ?? Google_Cloud_Speech_V1beta1_RecognitionConfig()}
    set {_uniqueStorage()._config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return _storage._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {_uniqueStorage()._config = nil}

  /// *Optional* If `false` or omitted, the recognizer will perform continuous
  /// recognition (continuing to wait for and process audio even if the user
  /// pauses speaking) until the client closes the input stream (gRPC API) or
  /// until the maximum time limit has been reached. May return multiple
  /// `StreamingRecognitionResult`s with the `is_final` flag set to `true`.
  ///
  /// If `true`, the recognizer will detect a single spoken utterance. When it
  /// detects that the user has paused or stopped speaking, it will return an
  /// `END_OF_UTTERANCE` event and cease recognition. It will return no more than
  /// one `StreamingRecognitionResult` with the `is_final` flag set to `true`.
  public var singleUtterance: Bool {
    get {return _storage._singleUtterance}
    set {_uniqueStorage()._singleUtterance = newValue}
  }

  /// *Optional* If `true`, interim results (tentative hypotheses) may be
  /// returned as they become available (these interim results are indicated with
  /// the `is_final=false` flag).
  /// If `false` or omitted, only `is_final=true` result(s) are returned.
  public var interimResults: Bool {
    get {return _storage._interimResults}
    set {_uniqueStorage()._interimResults = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Provides information to the recognizer that specifies how to process the
/// request.
public struct Google_Cloud_Speech_V1beta1_RecognitionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
  public var encoding: Google_Cloud_Speech_V1beta1_RecognitionConfig.AudioEncoding {
    get {return _storage._encoding}
    set {_uniqueStorage()._encoding = newValue}
  }

  /// *Required* Sample rate in Hertz of the audio data sent in all
  /// `RecognitionAudio` messages. Valid values are: 8000-48000.
  /// 16000 is optimal. For best results, set the sampling rate of the audio
  /// source to 16000 Hz. If that's not possible, use the native sample rate of
  /// the audio source (instead of re-sampling).
  public var sampleRate: Int32 {
    get {return _storage._sampleRate}
    set {_uniqueStorage()._sampleRate = newValue}
  }

  /// *Optional* The language of the supplied audio as a BCP-47 language tag.
  /// Example: "en-GB"  https://www.rfc-editor.org/rfc/bcp/bcp47.txt
  /// If omitted, defaults to "en-US". See
  /// [Language Support](https://cloud.google.com/speech/docs/languages)
  /// for a list of the currently supported language codes.
  public var languageCode: String {
    get {return _storage._languageCode}
    set {_uniqueStorage()._languageCode = newValue}
  }

  /// *Optional* Maximum number of recognition hypotheses to be returned.
  /// Specifically, the maximum number of `SpeechRecognitionAlternative` messages
  /// within each `SpeechRecognitionResult`.
  /// The server may return fewer than `max_alternatives`.
  /// Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
  /// one. If omitted, will return a maximum of one.
  public var maxAlternatives: Int32 {
    get {return _storage._maxAlternatives}
    set {_uniqueStorage()._maxAlternatives = newValue}
  }

  /// *Optional* If set to `true`, the server will attempt to filter out
  /// profanities, replacing all but the initial character in each filtered word
  /// with asterisks, e.g. "f***". If set to `false` or omitted, profanities
  /// won't be filtered out.
  public var profanityFilter: Bool {
    get {return _storage._profanityFilter}
    set {_uniqueStorage()._profanityFilter = newValue}
  }

  /// *Optional* A means to provide context to assist the speech recognition.
  public var speechContext: Google_Cloud_Speech_V1beta1_SpeechContext {
    get {return _storage._speechContext ?? Google_Cloud_Speech_V1beta1_SpeechContext()}
    set {_uniqueStorage()._speechContext = newValue}
  }
  /// Returns true if `speechContext` has been explicitly set.
  public var hasSpeechContext: Bool {return _storage._speechContext != nil}
  /// Clears the value of `speechContext`. Subsequent reads from it will return its default value.
  public mutating func clearSpeechContext() {_uniqueStorage()._speechContext = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Audio encoding of the data sent in the audio message. All encodings support
  /// only 1 channel (mono) audio. Only `FLAC` includes a header that describes
  /// the bytes of audio that follow the header. The other encodings are raw
  /// audio bytes with no header.
  ///
  /// For best results, the audio source should be captured and transmitted using
  /// a lossless encoding (`FLAC` or `LINEAR16`). Recognition accuracy may be
  /// reduced if lossy codecs (such as AMR, AMR_WB and MULAW) are used to capture
  /// or transmit the audio, particularly if background noise is present.
  public enum AudioEncoding: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
    case encodingUnspecified // = 0

    /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
    /// This is the only encoding that may be used by `AsyncRecognize`.
    case linear16 // = 1

    /// This is the recommended encoding for `SyncRecognize` and
    /// `StreamingRecognize` because it uses lossless compression; therefore
    /// recognition accuracy is not compromised by a lossy codec.
    ///
    /// The stream FLAC (Free Lossless Audio Codec) encoding is specified at:
    /// http://flac.sourceforge.net/documentation.html.
    /// 16-bit and 24-bit samples are supported.
    /// Not all fields in STREAMINFO are supported.
    case flac // = 2

    /// 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
    case mulaw // = 3

    /// Adaptive Multi-Rate Narrowband codec. `sample_rate` must be 8000 Hz.
    case amr // = 4

    /// Adaptive Multi-Rate Wideband codec. `sample_rate` must be 16000 Hz.
    case amrWb // = 5
    case UNRECOGNIZED(Int)

    public init() {
      self = .encodingUnspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .encodingUnspecified
      case 1: self = .linear16
      case 2: self = .flac
      case 3: self = .mulaw
      case 4: self = .amr
      case 5: self = .amrWb
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .encodingUnspecified: return 0
      case .linear16: return 1
      case .flac: return 2
      case .mulaw: return 3
      case .amr: return 4
      case .amrWb: return 5
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

#if swift(>=4.2)

extension Google_Cloud_Speech_V1beta1_RecognitionConfig.AudioEncoding: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1beta1_RecognitionConfig.AudioEncoding] = [
    .encodingUnspecified,
    .linear16,
    .flac,
    .mulaw,
    .amr,
    .amrWb,
  ]
}

#endif  // swift(>=4.2)

/// Provides "hints" to the speech recognizer to favor specific words and phrases
/// in the results.
public struct Google_Cloud_Speech_V1beta1_SpeechContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Optional* A list of strings containing words and phrases "hints" so that
  /// the speech recognition is more likely to recognize them. This can be used
  /// to improve the accuracy for specific words and phrases, for example, if
  /// specific commands are typically spoken by the user. This can also be used
  /// to add additional words to the vocabulary of the recognizer. See
  /// [usage limits](https://cloud.google.com/speech/limits#content).
  public var phrases: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Contains audio data in the encoding specified in the `RecognitionConfig`.
/// Either `content` or `uri` must be supplied. Supplying both or neither
/// returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
/// [audio limits](https://cloud.google.com/speech/limits#content).
public struct Google_Cloud_Speech_V1beta1_RecognitionAudio {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The audio source, which is either inline content or a GCS uri.
  public var audioSource: Google_Cloud_Speech_V1beta1_RecognitionAudio.OneOf_AudioSource? = nil

  /// The audio data bytes encoded as specified in
  /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
  /// pure binary representation, whereas JSON representations use base64.
  public var content: Data {
    get {
      if case .content(let v)? = audioSource {return v}
      return SwiftProtobuf.Internal.emptyData
    }
    set {audioSource = .content(newValue)}
  }

  /// URI that points to a file that contains audio data bytes as specified in
  /// `RecognitionConfig`. Currently, only Google Cloud Storage URIs are
  /// supported, which must be specified in the following format:
  /// `gs://bucket_name/object_name` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
  /// [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
  public var uri: String {
    get {
      if case .uri(let v)? = audioSource {return v}
      return String()
    }
    set {audioSource = .uri(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The audio source, which is either inline content or a GCS uri.
  public enum OneOf_AudioSource: Equatable {
    /// The audio data bytes encoded as specified in
    /// `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
    /// pure binary representation, whereas JSON representations use base64.
    case content(Data)
    /// URI that points to a file that contains audio data bytes as specified in
    /// `RecognitionConfig`. Currently, only Google Cloud Storage URIs are
    /// supported, which must be specified in the following format:
    /// `gs://bucket_name/object_name` (other URI formats return
    /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
    /// [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
    case uri(String)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Speech_V1beta1_RecognitionAudio.OneOf_AudioSource, rhs: Google_Cloud_Speech_V1beta1_RecognitionAudio.OneOf_AudioSource) -> Bool {
      switch (lhs, rhs) {
      case (.content(let l), .content(let r)): return l == r
      case (.uri(let l), .uri(let r)): return l == r
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// The only message returned to the client by `SyncRecognize`. method. It
/// contains the result as zero or more sequential `SpeechRecognitionResult`
/// messages.
public struct Google_Cloud_Speech_V1beta1_SyncRecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* Sequential list of transcription results corresponding to
  /// sequential portions of audio.
  public var results: [Google_Cloud_Speech_V1beta1_SpeechRecognitionResult] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The only message returned to the client by `AsyncRecognize`. It contains the
/// result as zero or more sequential `SpeechRecognitionResult` messages. It is
/// included in the `result.response` field of the `Operation` returned by the
/// `GetOperation` call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Speech_V1beta1_AsyncRecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* Sequential list of transcription results corresponding to
  /// sequential portions of audio.
  public var results: [Google_Cloud_Speech_V1beta1_SpeechRecognitionResult] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Describes the progress of a long-running `AsyncRecognize` call. It is
/// included in the `metadata` field of the `Operation` returned by the
/// `GetOperation` call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Speech_V1beta1_AsyncRecognizeMetadata {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Approximate percentage of audio processed thus far. Guaranteed to be 100
  /// when the audio is fully processed and the results are available.
  public var progressPercent: Int32 {
    get {return _storage._progressPercent}
    set {_uniqueStorage()._progressPercent = newValue}
  }

  /// Time when the request was received.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Time of the most recent processing update.
  public var lastUpdateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._lastUpdateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._lastUpdateTime = newValue}
  }
  /// Returns true if `lastUpdateTime` has been explicitly set.
  public var hasLastUpdateTime: Bool {return _storage._lastUpdateTime != nil}
  /// Clears the value of `lastUpdateTime`. Subsequent reads from it will return its default value.
  public mutating func clearLastUpdateTime() {_uniqueStorage()._lastUpdateTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// `StreamingRecognizeResponse` is the only message returned to the client by
/// `StreamingRecognize`. A series of one or more `StreamingRecognizeResponse`
/// messages are streamed back to the client.
///
/// Here's an example of a series of ten `StreamingRecognizeResponse`s that might
/// be returned while processing audio:
///
/// 1. endpointer_type: START_OF_SPEECH
///
/// 2. results { alternatives { transcript: "tube" } stability: 0.01 }
///    result_index: 0
///
/// 3. results { alternatives { transcript: "to be a" } stability: 0.01 }
///    result_index: 0
///
/// 4. results { alternatives { transcript: "to be" } stability: 0.9 }
///    results { alternatives { transcript: " or not to be" } stability: 0.01 }
///    result_index: 0
///
/// 5. results { alternatives { transcript: "to be or not to be"
///                             confidence: 0.92 }
///              alternatives { transcript: "to bee or not to bee" }
///              is_final: true }
///    result_index: 0
///
/// 6. results { alternatives { transcript: " that's" } stability: 0.01 }
///    result_index: 1
///
/// 7. results { alternatives { transcript: " that is" } stability: 0.9 }
///    results { alternatives { transcript: " the question" } stability: 0.01 }
///    result_index: 1
///
/// 8. endpointer_type: END_OF_SPEECH
///
/// 9. results { alternatives { transcript: " that is the question"
///                             confidence: 0.98 }
///              alternatives { transcript: " that was the question" }
///              is_final: true }
///    result_index: 1
///
/// 10. endpointer_type: END_OF_AUDIO
///
/// Notes:
///
/// - Only two of the above responses #5 and #9 contain final results, they are
///   indicated by `is_final: true`. Concatenating these together generates the
///   full transcript: "to be or not to be that is the question".
///
/// - The others contain interim `results`. #4 and #7 contain two interim
///   `results`, the first portion has a high stability and is less likely to
///   change, the second portion has a low stability and is very likely to
///   change. A UI designer might choose to show only high stability `results`.
///
/// - The specific `stability` and `confidence` values shown above are only for
///   illustrative purposes. Actual values may vary.
///
/// - The `result_index` indicates the portion of audio that has had final
///   results returned, and is no longer being processed. For example, the
///   `results` in #6 and later correspond to the portion of audio after
///   "to be or not to be".
public struct Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* If set, returns a [google.rpc.Status][google.rpc.Status] message that
  /// specifies the error for the operation.
  public var error: GoogleRPCModel.Google_Rpc_Status {
    get {return _storage._error ?? GoogleRPCModel.Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// *Output-only* This repeated list contains zero or more results that
  /// correspond to consecutive portions of the audio currently being processed.
  /// It contains zero or one `is_final=true` result (the newly settled portion),
  /// followed by zero or more `is_final=false` results.
  public var results: [Google_Cloud_Speech_V1beta1_StreamingRecognitionResult] {
    get {return _storage._results}
    set {_uniqueStorage()._results = newValue}
  }

  /// *Output-only* Indicates the lowest index in the `results` array that has
  /// changed. The repeated `StreamingRecognitionResult` results overwrite past
  /// results at this index and higher.
  public var resultIndex: Int32 {
    get {return _storage._resultIndex}
    set {_uniqueStorage()._resultIndex = newValue}
  }

  /// *Output-only* Indicates the type of endpointer event.
  public var endpointerType: Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse.EndpointerType {
    get {return _storage._endpointerType}
    set {_uniqueStorage()._endpointerType = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates the type of endpointer event.
  public enum EndpointerType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// No endpointer event specified.
    case endpointerEventUnspecified // = 0

    /// Speech has been detected in the audio stream, and the service is
    /// beginning to process it.
    case startOfSpeech // = 1

    /// Speech has ceased to be detected in the audio stream. (For example, the
    /// user may have paused after speaking.) If `single_utterance` is `false`,
    /// the service will continue to process audio, and if subsequent speech is
    /// detected, will send another START_OF_SPEECH event.
    case endOfSpeech // = 2

    /// This event is sent after the client has half-closed the input stream gRPC
    /// connection and the server has received all of the audio. (The server may
    /// still be processing the audio and may subsequently return additional
    /// results.)
    case endOfAudio // = 3

    /// This event is only sent when `single_utterance` is `true`. It indicates
    /// that the server has detected the end of the user's speech utterance and
    /// expects no additional speech. Therefore, the server will not process
    /// additional audio (although it may subsequently return additional
    /// results). The client should stop sending additional audio data,
    /// half-close the gRPC connection, and wait for any additional results
    /// until the server closes the gRPC connection.
    case endOfUtterance // = 4
    case UNRECOGNIZED(Int)

    public init() {
      self = .endpointerEventUnspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .endpointerEventUnspecified
      case 1: self = .startOfSpeech
      case 2: self = .endOfSpeech
      case 3: self = .endOfAudio
      case 4: self = .endOfUtterance
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .endpointerEventUnspecified: return 0
      case .startOfSpeech: return 1
      case .endOfSpeech: return 2
      case .endOfAudio: return 3
      case .endOfUtterance: return 4
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

#if swift(>=4.2)

extension Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse.EndpointerType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse.EndpointerType] = [
    .endpointerEventUnspecified,
    .startOfSpeech,
    .endOfSpeech,
    .endOfAudio,
    .endOfUtterance,
  ]
}

#endif  // swift(>=4.2)

/// A streaming speech recognition result corresponding to a portion of the audio
/// that is currently being processed.
public struct Google_Cloud_Speech_V1beta1_StreamingRecognitionResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* May contain one or more recognition hypotheses (up to the
  /// maximum specified in `max_alternatives`).
  public var alternatives: [Google_Cloud_Speech_V1beta1_SpeechRecognitionAlternative] = []

  /// *Output-only* If `false`, this `StreamingRecognitionResult` represents an
  /// interim result that may change. If `true`, this is the final time the
  /// speech service will return this particular `StreamingRecognitionResult`,
  /// the recognizer will not return any further hypotheses for this portion of
  /// the transcript and corresponding audio.
  public var isFinal: Bool = false

  /// *Output-only* An estimate of the likelihood that the recognizer will not
  /// change its guess about this interim result. Values range from 0.0
  /// (completely unstable) to 1.0 (completely stable).
  /// This field is only provided for interim results (`is_final=false`).
  /// The default of 0.0 is a sentinel value indicating `stability` was not set.
  public var stability: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A speech recognition result corresponding to a portion of the audio.
public struct Google_Cloud_Speech_V1beta1_SpeechRecognitionResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* May contain one or more recognition hypotheses (up to the
  /// maximum specified in `max_alternatives`).
  public var alternatives: [Google_Cloud_Speech_V1beta1_SpeechRecognitionAlternative] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Alternative hypotheses (a.k.a. n-best list).
public struct Google_Cloud_Speech_V1beta1_SpeechRecognitionAlternative {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* Transcript text representing the words that the user spoke.
  public var transcript: String = String()

  /// *Output-only* The confidence estimate between 0.0 and 1.0. A higher number
  /// indicates an estimated greater likelihood that the recognized words are
  /// correct. This field is typically provided only for the top hypothesis, and
  /// only for `is_final=true` results. Clients should not rely on the
  /// `confidence` field as it is not guaranteed to be accurate, or even set, in
  /// any of the results.
  /// The default of 0.0 is a sentinel value indicating `confidence` was not set.
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.speech.v1beta1"

extension Google_Cloud_Speech_V1beta1_SyncRecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SyncRecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .same(proto: "audio"),
  ]

  fileprivate class _StorageClass {
    var _config: Google_Cloud_Speech_V1beta1_RecognitionConfig? = nil
    var _audio: Google_Cloud_Speech_V1beta1_RecognitionAudio? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _config = source._config
      _audio = source._audio
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._config)
        case 2: try decoder.decodeSingularMessageField(value: &_storage._audio)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._config {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if let v = _storage._audio {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_SyncRecognizeRequest, rhs: Google_Cloud_Speech_V1beta1_SyncRecognizeRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._config != rhs_storage._config {return false}
        if _storage._audio != rhs_storage._audio {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_AsyncRecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AsyncRecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .same(proto: "audio"),
  ]

  fileprivate class _StorageClass {
    var _config: Google_Cloud_Speech_V1beta1_RecognitionConfig? = nil
    var _audio: Google_Cloud_Speech_V1beta1_RecognitionAudio? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _config = source._config
      _audio = source._audio
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._config)
        case 2: try decoder.decodeSingularMessageField(value: &_storage._audio)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._config {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if let v = _storage._audio {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_AsyncRecognizeRequest, rhs: Google_Cloud_Speech_V1beta1_AsyncRecognizeRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._config != rhs_storage._config {return false}
        if _storage._audio != rhs_storage._audio {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_StreamingRecognizeRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognizeRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "streaming_config"),
    2: .standard(proto: "audio_content"),
  ]

  fileprivate class _StorageClass {
    var _streamingRequest: Google_Cloud_Speech_V1beta1_StreamingRecognizeRequest.OneOf_StreamingRequest?

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _streamingRequest = source._streamingRequest
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1:
          var v: Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig?
          if let current = _storage._streamingRequest {
            try decoder.handleConflictingOneOf()
            if case .streamingConfig(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._streamingRequest = .streamingConfig(v)}
        case 2:
          if _storage._streamingRequest != nil {try decoder.handleConflictingOneOf()}
          var v: Data?
          try decoder.decodeSingularBytesField(value: &v)
          if let v = v {_storage._streamingRequest = .audioContent(v)}
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      switch _storage._streamingRequest {
      case .streamingConfig(let v)?:
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      case .audioContent(let v)?:
        try visitor.visitSingularBytesField(value: v, fieldNumber: 2)
      case nil: break
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_StreamingRecognizeRequest, rhs: Google_Cloud_Speech_V1beta1_StreamingRecognizeRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._streamingRequest != rhs_storage._streamingRequest {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognitionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .standard(proto: "single_utterance"),
    3: .standard(proto: "interim_results"),
  ]

  fileprivate class _StorageClass {
    var _config: Google_Cloud_Speech_V1beta1_RecognitionConfig? = nil
    var _singleUtterance: Bool = false
    var _interimResults: Bool = false

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _config = source._config
      _singleUtterance = source._singleUtterance
      _interimResults = source._interimResults
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._config)
        case 2: try decoder.decodeSingularBoolField(value: &_storage._singleUtterance)
        case 3: try decoder.decodeSingularBoolField(value: &_storage._interimResults)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._config {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if _storage._singleUtterance != false {
        try visitor.visitSingularBoolField(value: _storage._singleUtterance, fieldNumber: 2)
      }
      if _storage._interimResults != false {
        try visitor.visitSingularBoolField(value: _storage._interimResults, fieldNumber: 3)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig, rhs: Google_Cloud_Speech_V1beta1_StreamingRecognitionConfig) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._config != rhs_storage._config {return false}
        if _storage._singleUtterance != rhs_storage._singleUtterance {return false}
        if _storage._interimResults != rhs_storage._interimResults {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_RecognitionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognitionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "encoding"),
    2: .standard(proto: "sample_rate"),
    3: .standard(proto: "language_code"),
    4: .standard(proto: "max_alternatives"),
    5: .standard(proto: "profanity_filter"),
    6: .standard(proto: "speech_context"),
  ]

  fileprivate class _StorageClass {
    var _encoding: Google_Cloud_Speech_V1beta1_RecognitionConfig.AudioEncoding = .encodingUnspecified
    var _sampleRate: Int32 = 0
    var _languageCode: String = String()
    var _maxAlternatives: Int32 = 0
    var _profanityFilter: Bool = false
    var _speechContext: Google_Cloud_Speech_V1beta1_SpeechContext? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _encoding = source._encoding
      _sampleRate = source._sampleRate
      _languageCode = source._languageCode
      _maxAlternatives = source._maxAlternatives
      _profanityFilter = source._profanityFilter
      _speechContext = source._speechContext
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularEnumField(value: &_storage._encoding)
        case 2: try decoder.decodeSingularInt32Field(value: &_storage._sampleRate)
        case 3: try decoder.decodeSingularStringField(value: &_storage._languageCode)
        case 4: try decoder.decodeSingularInt32Field(value: &_storage._maxAlternatives)
        case 5: try decoder.decodeSingularBoolField(value: &_storage._profanityFilter)
        case 6: try decoder.decodeSingularMessageField(value: &_storage._speechContext)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if _storage._encoding != .encodingUnspecified {
        try visitor.visitSingularEnumField(value: _storage._encoding, fieldNumber: 1)
      }
      if _storage._sampleRate != 0 {
        try visitor.visitSingularInt32Field(value: _storage._sampleRate, fieldNumber: 2)
      }
      if !_storage._languageCode.isEmpty {
        try visitor.visitSingularStringField(value: _storage._languageCode, fieldNumber: 3)
      }
      if _storage._maxAlternatives != 0 {
        try visitor.visitSingularInt32Field(value: _storage._maxAlternatives, fieldNumber: 4)
      }
      if _storage._profanityFilter != false {
        try visitor.visitSingularBoolField(value: _storage._profanityFilter, fieldNumber: 5)
      }
      if let v = _storage._speechContext {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_RecognitionConfig, rhs: Google_Cloud_Speech_V1beta1_RecognitionConfig) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._encoding != rhs_storage._encoding {return false}
        if _storage._sampleRate != rhs_storage._sampleRate {return false}
        if _storage._languageCode != rhs_storage._languageCode {return false}
        if _storage._maxAlternatives != rhs_storage._maxAlternatives {return false}
        if _storage._profanityFilter != rhs_storage._profanityFilter {return false}
        if _storage._speechContext != rhs_storage._speechContext {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_RecognitionConfig.AudioEncoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ENCODING_UNSPECIFIED"),
    1: .same(proto: "LINEAR16"),
    2: .same(proto: "FLAC"),
    3: .same(proto: "MULAW"),
    4: .same(proto: "AMR"),
    5: .same(proto: "AMR_WB"),
  ]
}

extension Google_Cloud_Speech_V1beta1_SpeechContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "phrases"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedStringField(value: &self.phrases)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.phrases.isEmpty {
      try visitor.visitRepeatedStringField(value: self.phrases, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_SpeechContext, rhs: Google_Cloud_Speech_V1beta1_SpeechContext) -> Bool {
    if lhs.phrases != rhs.phrases {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_RecognitionAudio: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RecognitionAudio"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "content"),
    2: .same(proto: "uri"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1:
        if self.audioSource != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.audioSource = .content(v)}
      case 2:
        if self.audioSource != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.audioSource = .uri(v)}
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    switch self.audioSource {
    case .content(let v)?:
      try visitor.visitSingularBytesField(value: v, fieldNumber: 1)
    case .uri(let v)?:
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_RecognitionAudio, rhs: Google_Cloud_Speech_V1beta1_RecognitionAudio) -> Bool {
    if lhs.audioSource != rhs.audioSource {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_SyncRecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SyncRecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 2: try decoder.decodeRepeatedMessageField(value: &self.results)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.results.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.results, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_SyncRecognizeResponse, rhs: Google_Cloud_Speech_V1beta1_SyncRecognizeResponse) -> Bool {
    if lhs.results != rhs.results {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_AsyncRecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AsyncRecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 2: try decoder.decodeRepeatedMessageField(value: &self.results)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.results.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.results, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_AsyncRecognizeResponse, rhs: Google_Cloud_Speech_V1beta1_AsyncRecognizeResponse) -> Bool {
    if lhs.results != rhs.results {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_AsyncRecognizeMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AsyncRecognizeMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "progress_percent"),
    2: .standard(proto: "start_time"),
    3: .standard(proto: "last_update_time"),
  ]

  fileprivate class _StorageClass {
    var _progressPercent: Int32 = 0
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _lastUpdateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _progressPercent = source._progressPercent
      _startTime = source._startTime
      _lastUpdateTime = source._lastUpdateTime
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularInt32Field(value: &_storage._progressPercent)
        case 2: try decoder.decodeSingularMessageField(value: &_storage._startTime)
        case 3: try decoder.decodeSingularMessageField(value: &_storage._lastUpdateTime)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if _storage._progressPercent != 0 {
        try visitor.visitSingularInt32Field(value: _storage._progressPercent, fieldNumber: 1)
      }
      if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
      if let v = _storage._lastUpdateTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_AsyncRecognizeMetadata, rhs: Google_Cloud_Speech_V1beta1_AsyncRecognizeMetadata) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._progressPercent != rhs_storage._progressPercent {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._lastUpdateTime != rhs_storage._lastUpdateTime {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognizeResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "error"),
    2: .same(proto: "results"),
    3: .standard(proto: "result_index"),
    4: .standard(proto: "endpointer_type"),
  ]

  fileprivate class _StorageClass {
    var _error: GoogleRPCModel.Google_Rpc_Status? = nil
    var _results: [Google_Cloud_Speech_V1beta1_StreamingRecognitionResult] = []
    var _resultIndex: Int32 = 0
    var _endpointerType: Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse.EndpointerType = .endpointerEventUnspecified

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _error = source._error
      _results = source._results
      _resultIndex = source._resultIndex
      _endpointerType = source._endpointerType
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        switch fieldNumber {
        case 1: try decoder.decodeSingularMessageField(value: &_storage._error)
        case 2: try decoder.decodeRepeatedMessageField(value: &_storage._results)
        case 3: try decoder.decodeSingularInt32Field(value: &_storage._resultIndex)
        case 4: try decoder.decodeSingularEnumField(value: &_storage._endpointerType)
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if !_storage._results.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._results, fieldNumber: 2)
      }
      if _storage._resultIndex != 0 {
        try visitor.visitSingularInt32Field(value: _storage._resultIndex, fieldNumber: 3)
      }
      if _storage._endpointerType != .endpointerEventUnspecified {
        try visitor.visitSingularEnumField(value: _storage._endpointerType, fieldNumber: 4)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse, rhs: Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._error != rhs_storage._error {return false}
        if _storage._results != rhs_storage._results {return false}
        if _storage._resultIndex != rhs_storage._resultIndex {return false}
        if _storage._endpointerType != rhs_storage._endpointerType {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_StreamingRecognizeResponse.EndpointerType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ENDPOINTER_EVENT_UNSPECIFIED"),
    1: .same(proto: "START_OF_SPEECH"),
    2: .same(proto: "END_OF_SPEECH"),
    3: .same(proto: "END_OF_AUDIO"),
    4: .same(proto: "END_OF_UTTERANCE"),
  ]
}

extension Google_Cloud_Speech_V1beta1_StreamingRecognitionResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingRecognitionResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
    2: .standard(proto: "is_final"),
    3: .same(proto: "stability"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedMessageField(value: &self.alternatives)
      case 2: try decoder.decodeSingularBoolField(value: &self.isFinal)
      case 3: try decoder.decodeSingularFloatField(value: &self.stability)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    if self.isFinal != false {
      try visitor.visitSingularBoolField(value: self.isFinal, fieldNumber: 2)
    }
    if self.stability != 0 {
      try visitor.visitSingularFloatField(value: self.stability, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_StreamingRecognitionResult, rhs: Google_Cloud_Speech_V1beta1_StreamingRecognitionResult) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.isFinal != rhs.isFinal {return false}
    if lhs.stability != rhs.stability {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_SpeechRecognitionResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeRepeatedMessageField(value: &self.alternatives)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_SpeechRecognitionResult, rhs: Google_Cloud_Speech_V1beta1_SpeechRecognitionResult) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Speech_V1beta1_SpeechRecognitionAlternative: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionAlternative"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "transcript"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      switch fieldNumber {
      case 1: try decoder.decodeSingularStringField(value: &self.transcript)
      case 2: try decoder.decodeSingularFloatField(value: &self.confidence)
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.transcript.isEmpty {
      try visitor.visitSingularStringField(value: self.transcript, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Speech_V1beta1_SpeechRecognitionAlternative, rhs: Google_Cloud_Speech_V1beta1_SpeechRecognitionAlternative) -> Bool {
    if lhs.transcript != rhs.transcript {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
